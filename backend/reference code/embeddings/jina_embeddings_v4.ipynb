{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FsR2fy851u43"
      },
      "outputs": [],
      "source": [
        "# !pip install -U transformers peft torch ipywidgets jupyter Torchvision\n",
        "# !pip install transformers>=4.52.0 torch>=2.6.0 peft>=0.15.2 torchvision pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnerPObQ1u45"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/jinaai/jina-embeddings-v4\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/jinaai/jina-embeddings-v4)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1muser: \u001b[0m MukeshDevrath007\n"
          ]
        }
      ],
      "source": [
        "!hf auth whoami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: False\n",
            "MPS available: True\n",
            "Device: mps\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "import torch\n",
        "\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"MPS available:\", torch.backends.mps.is_available())\n",
        "print(\"Device:\", \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "269a789856cb4803b3ef7b29ebc066c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e1b4e0bb33c4770a299f7223b8f8fea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13a26fe343644583bf98da4bb460555d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForFeatureExtraction(\n",
              "  (base_model): LoraModel(\n",
              "    (model): JinaEmbeddingsV4Model(\n",
              "      (model): Qwen2_5_VLModel(\n",
              "        (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
              "          (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
              "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "          )\n",
              "          (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
              "          (blocks): ModuleList(\n",
              "            (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
              "              (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
              "              (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
              "              (attn): Qwen2_5_VLVisionSdpaAttention(\n",
              "                (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "                (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "              )\n",
              "              (mlp): Qwen2_5_VLMLP(\n",
              "                (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
              "                (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
              "                (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
              "                (act_fn): SiLUActivation()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (merger): Qwen2_5_VLPatchMerger(\n",
              "            (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
              "            (mlp): Sequential(\n",
              "              (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (language_model): Qwen2_5_VLTextModel(\n",
              "          (embed_tokens): Embedding(151936, 2048)\n",
              "          (layers): ModuleList(\n",
              "            (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
              "              (self_attn): Qwen2_5_VLSdpaAttention(\n",
              "                (q_proj): lora.MultiAdapterLinear(\n",
              "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (text-matching): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (code): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                      (text-matching): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                      (code): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (k_proj): lora.MultiAdapterLinear(\n",
              "                  (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (text-matching): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (code): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=32, out_features=256, bias=False)\n",
              "                      (text-matching): Linear(in_features=32, out_features=256, bias=False)\n",
              "                      (code): Linear(in_features=32, out_features=256, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (v_proj): lora.MultiAdapterLinear(\n",
              "                  (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (text-matching): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (code): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=32, out_features=256, bias=False)\n",
              "                      (text-matching): Linear(in_features=32, out_features=256, bias=False)\n",
              "                      (code): Linear(in_features=32, out_features=256, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (o_proj): lora.MultiAdapterLinear(\n",
              "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (text-matching): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (code): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                      (text-matching): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                      (code): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
              "              )\n",
              "              (mlp): Qwen2MLP(\n",
              "                (gate_proj): lora.MultiAdapterLinear(\n",
              "                  (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (text-matching): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (code): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                      (text-matching): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                      (code): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (up_proj): lora.MultiAdapterLinear(\n",
              "                  (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (text-matching): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                      (code): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                      (text-matching): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                      (code): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (down_proj): lora.MultiAdapterLinear(\n",
              "                  (base_layer): Linear(in_features=11008, out_features=2048, bias=False)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=11008, out_features=32, bias=False)\n",
              "                      (text-matching): Linear(in_features=11008, out_features=32, bias=False)\n",
              "                      (code): Linear(in_features=11008, out_features=32, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): ModuleDict(\n",
              "                      (retrieval): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                      (text-matching): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                      (code): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (act_fn): SiLUActivation()\n",
              "              )\n",
              "              (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
              "              (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
              "            )\n",
              "          )\n",
              "          (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
              "          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
              "      (multi_vector_projector): lora.MultiAdapterLinear(\n",
              "        (base_layer): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): ModuleDict(\n",
              "            (retrieval): Linear(in_features=2048, out_features=32, bias=False)\n",
              "            (text-matching): Linear(in_features=2048, out_features=32, bias=False)\n",
              "            (code): Linear(in_features=2048, out_features=32, bias=False)\n",
              "          )\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): ModuleDict(\n",
              "            (retrieval): Linear(in_features=32, out_features=128, bias=False)\n",
              "            (text-matching): Linear(in_features=32, out_features=128, bias=False)\n",
              "            (code): Linear(in_features=32, out_features=128, bias=False)\n",
              "          )\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the model\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v4\", trust_remote_code=True, dtype=torch.float16)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding texts...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:42<00:00, 42.43s/it]\n",
            "Encoding texts...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:23<00:00, 23.91s/it]\n",
            "Encoding images...:   0%|          | 0/1 [00:09<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Unexpected floating ScalarType in at::autocast::prioritize",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m passage_embeddings = model.encode_text(\n\u001b[32m     15\u001b[39m     texts=[\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mClimate change has led to rising sea levels, increased frequency of extreme weather events...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     prompt_name=\u001b[33m\"\u001b[39m\u001b[33mpassage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Encode image/document\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m image_embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://i.ibb.co/nQNGqL0/beach1.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_embeddings_hyphen_v4/737fa5c46f0262ceba4a462ffa1c5bcf01da416f/modeling_jina_embeddings_v4.py:542\u001b[39m, in \u001b[36mJinaEmbeddingsV4Model.encode_image\u001b[39m\u001b[34m(self, images, task, batch_size, return_multivector, return_numpy, truncate_dim, max_pixels)\u001b[39m\n\u001b[32m    539\u001b[39m     images = [images]\n\u001b[32m    541\u001b[39m images = \u001b[38;5;28mself\u001b[39m._load_images_if_needed(images)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessor_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEncoding images...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_multivector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_multivector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_numpy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_pixels:\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.processor.image_processor.max_pixels = default_max_pixels\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_embeddings_hyphen_v4/737fa5c46f0262ceba4a462ffa1c5bcf01da416f/modeling_jina_embeddings_v4.py:346\u001b[39m, in \u001b[36mJinaEmbeddingsV4Model._process_batches\u001b[39m\u001b[34m(self, data, task_label, processor_fn, desc, return_multivector, return_numpy, batch_size, truncate_dim)\u001b[39m\n\u001b[32m    342\u001b[39m batch = {k: v.to(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(\n\u001b[32m    344\u001b[39m     device_type=torch.device(\u001b[38;5;28mself\u001b[39m.device).type, dtype=torch.bfloat16\n\u001b[32m    345\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_multivector:\n\u001b[32m    348\u001b[39m         embeddings = embeddings.single_vec_emb\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_embeddings_hyphen_v4/737fa5c46f0262ceba4a462ffa1c5bcf01da416f/modeling_jina_embeddings_v4.py:291\u001b[39m, in \u001b[36mJinaEmbeddingsV4Model.forward\u001b[39m\u001b[34m(self, task_label, input_ids, attention_mask, output_vlm_last_hidden_states, **kwargs)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03mForward pass through the model. Returns both single-vector and multi-vector embeddings.\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    288\u001b[39m \u001b[33;03m        multi_vec_emb (torch.Tensor, optional): Multi-vector embeddings.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Forward pass through the VLM\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_last_hidden_states\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Compute the embeddings\u001b[39;00m\n\u001b[32m    298\u001b[39m single_vec_emb = \u001b[38;5;28mself\u001b[39m.get_single_vector_embeddings(\n\u001b[32m    299\u001b[39m     hidden_states=hidden_states,\n\u001b[32m    300\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    301\u001b[39m     input_ids=input_ids,\n\u001b[32m    302\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_embeddings_hyphen_v4/737fa5c46f0262ceba4a462ffa1c5bcf01da416f/modeling_jina_embeddings_v4.py:190\u001b[39m, in \u001b[36mJinaEmbeddingsV4Model.get_last_hidden_states\u001b[39m\u001b[34m(self, task_label, input_ids, attention_mask, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m position_ids, rope_deltas = \u001b[38;5;28mself\u001b[39m.model.get_rope_index(\n\u001b[32m    184\u001b[39m     input_ids=input_ids,\n\u001b[32m    185\u001b[39m     image_grid_thw=kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mimage_grid_thw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    186\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    187\u001b[39m )\n\u001b[32m    189\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_deltas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrope_deltas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m hidden_states = outputs.hidden_states\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hidden_states:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rag/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_embeddings_hyphen_v4/737fa5c46f0262ceba4a462ffa1c5bcf01da416f/qwen2_5_vl.py:2235\u001b[39m, in \u001b[36mQwen2_5_VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, task_label, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[39m\n\u001b[32m   2230\u001b[39m output_hidden_states = (\n\u001b[32m   2231\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   2232\u001b[39m )\n\u001b[32m   2233\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m2235\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2237\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m    \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m=\u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2252\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2254\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   2255\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_embeddings_hyphen_v4/737fa5c46f0262ceba4a462ffa1c5bcf01da416f/qwen2_5_vl.py:1996\u001b[39m, in \u001b[36mQwen2_5_VLModel.forward\u001b[39m\u001b[34m(self, task_label, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[39m\n\u001b[32m   1993\u001b[39m     image_mask = mask_expanded.to(inputs_embeds.device)\n\u001b[32m   1995\u001b[39m     image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m     inputs_embeds = \u001b[43minputs_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values_videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1999\u001b[39m     video_embeds = \u001b[38;5;28mself\u001b[39m.get_video_features(pixel_values_videos, video_grid_thw)\n",
            "\u001b[31mRuntimeError\u001b[39m: Unexpected floating ScalarType in at::autocast::prioritize"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# 1. Retrieval Task\n",
        "# ========================\n",
        "# Configure truncate_dim, max_length (for texts), max_pixels (for images), vector_type, batch_size in the encode function if needed\n",
        "\n",
        "# Encode query\n",
        "query_embeddings = model.encode_text(\n",
        "    texts=[\"Overview of climate change impacts on coastal cities\"],\n",
        "    task=\"retrieval\",\n",
        "    prompt_name=\"query\",\n",
        ")\n",
        "\n",
        "# Encode passage (text)\n",
        "passage_embeddings = model.encode_text(\n",
        "    texts=[\n",
        "        \"Climate change has led to rising sea levels, increased frequency of extreme weather events...\"\n",
        "    ],\n",
        "    task=\"retrieval\",\n",
        "    prompt_name=\"passage\",\n",
        ")\n",
        "\n",
        "# Encode image/document\n",
        "image_embeddings = model.encode_image(\n",
        "    images=[\"https://i.ibb.co/nQNGqL0/beach1.jpg\"],\n",
        "    task=\"retrieval\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 2. Text Matching Task\n",
        "# ========================\n",
        "texts = [\n",
        "    \"ÿ∫ÿ±Ÿàÿ® ÿ¨ŸÖŸäŸÑ ÿπŸÑŸâ ÿßŸÑÿ¥ÿßÿ∑ÿ¶\",  # Arabic\n",
        "    \"Êµ∑Êª©‰∏äÁæé‰∏ΩÁöÑÊó•ËêΩ\",  # Chinese\n",
        "    \"Un beau coucher de soleil sur la plage\",  # French\n",
        "    \"Ein wundersch√∂ner Sonnenuntergang am Strand\",  # German\n",
        "    \"ŒàŒΩŒ± œåŒºŒøœÅœÜŒø Œ∑ŒªŒπŒøŒ≤Œ±œÉŒØŒªŒµŒºŒ± œÄŒ¨ŒΩœâ Œ±œÄœå œÑŒ∑ŒΩ œÄŒ±œÅŒ±ŒªŒØŒ±\",  # Greek\n",
        "    \"‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§§‡§ü ‡§™‡§∞ ‡§è‡§ï ‡§ñ‡•Ç‡§¨‡§∏‡•Ç‡§∞‡§§ ‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§æ‡§∏‡•ç‡§§\",  # Hindi\n",
        "    \"Un bellissimo tramonto sulla spiaggia\",  # Italian\n",
        "    \"ÊµúËæ∫„Å´Ê≤à„ÇÄÁæé„Åó„ÅÑÂ§ïÊó•\",  # Japanese\n",
        "    \"Ìï¥Î≥Ä ÏúÑÎ°ú ÏïÑÎ¶ÑÎã§Ïö¥ ÏùºÎ™∞\",  # Korean\n",
        "]\n",
        "\n",
        "text_embeddings = model.encode_text(texts=texts, task=\"text-matching\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 3. Code Understanding Task\n",
        "# ========================\n",
        "\n",
        "# Encode query\n",
        "query_embedding = model.encode_text(\n",
        "    texts=[\"Find a function that prints a greeting message to the console\"],\n",
        "    task=\"code\",\n",
        "    prompt_name=\"query\",\n",
        ")\n",
        "\n",
        "# Encode code\n",
        "code_embeddings = model.encode_text(\n",
        "    texts=[\"def hello_world():\\n    print('Hello, World!')\"],\n",
        "    task=\"code\",\n",
        "    prompt_name=\"passage\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 4. Use multivectors\n",
        "# ========================\n",
        "\n",
        "multivector_embeddings = model.encode_text(\n",
        "    texts=texts,\n",
        "    task=\"retrieval\",\n",
        "    prompt_name=\"query\",\n",
        "    return_multivector=True,\n",
        ")\n",
        "\n",
        "images = [\"https://i.ibb.co/nQNGqL0/beach1.jpg\", \"https://i.ibb.co/r5w8hG8/beach2.jpg\"]\n",
        "multivector_image_embeddings = model.encode_image(\n",
        "    images=images,\n",
        "    task=\"retrieval\",\n",
        "    return_multivector=True,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
